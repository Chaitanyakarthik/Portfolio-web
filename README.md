# üëã Hi, I'm Chaitanya Karthik  
### Entry-Level Data Engineer | Azure ‚Ä¢ Databricks ‚Ä¢ Microsoft Fabric

Welcome to my GitHub profile.

I am an entry-level Data Engineer with hands-on experience supporting cloud data pipelines, Lakehouse tables, and analytics-ready datasets using **Azure** and **Microsoft Fabric**.

I focus on building scalable ETL workflows, improving data reliability, and designing enterprise-ready data solutions.

---

## üéì Education

**Oklahoma State University (OSU)**  
Master of Science in Management Information Systems  
GPA: 4.0 / 4.0  

Relevant Coursework:  
Programming for Data Science ‚Ä¢ Advanced Data Wrangling ‚Ä¢ Predictive Analytics ‚Ä¢ Descriptive Analytics ‚Ä¢ Enterprise Systems

---

## üíº Professional Experience

### **Data Engineer**  
Sonata Software ‚Äì Microsoft Partner  

‚Ä¢ Supported and enhanced Azure Data Factory pipelines ingesting structured and semi-structured data (JSON / CSV) from REST APIs (Microsoft Dynamics 365)  
‚Ä¢ Maintained ETL workflows across ADLS Gen2 and Azure SQL, improving data reliability and reporting performance  
‚Ä¢ Assisted in automating data operations using Azure Logic Apps, eliminating recurring manual processes  
‚Ä¢ Monitored pipelines and notebooks, debugged ingestion and transformation issues, and implemented logging/error handling  

---

### **Graduate Research Assistant**  
Oklahoma State University  

‚Ä¢ Built and maintained a 12K+ multi-source dataset via Python, REST APIs, and survey data  
‚Ä¢ Improved data quality by cleaning, validating, and standardizing attributes using Pandas and NumPy  
‚Ä¢ Documented validation rules and transformation logic for reproducibility and auditability  

---

## üöÄ Featured Projects

### ‚≠ê **Enterprise Azure Hybrid Data Engineering Platform**

Designed and implemented a hybrid Azure data platform simulating enterprise-grade cloud data workflows.

**Key Implementations:**

‚Ä¢ Built end-to-end ETL pipelines using Azure Data Factory  
‚Ä¢ Implemented hybrid ingestion via Self-Hosted Integration Runtime  
‚Ä¢ Designed cloud storage architecture using ADLS Gen2  
‚Ä¢ Performed distributed transformations using Azure Databricks (PySpark)  
‚Ä¢ Applied Delta Lake format for analytics optimization  
‚Ä¢ Implemented secure credential management using Azure Key Vault  
‚Ä¢ Configured monitoring & alerts using Azure Logic Apps  
‚Ä¢ Enabled CI/CD workflows via Git integration & ARM templates  

üîó Project Repository:  
https://github.com/Chaitanyakarthik/Enterprise-Azure-Hybrid-Dataplatform

---

### ‚≠ê **Microsoft Fabric Lakehouse Analytics Solution**

Developed a Lakehouse analytics solution using Microsoft Fabric and OneLake.

**Key Implementations:**

‚Ä¢ Consolidated multi-source datasets into fact and dimension tables  
‚Ä¢ Implemented Medallion Architecture processing using PySpark  
‚Ä¢ Stored analytics-ready datasets as Delta Lake tables  
‚Ä¢ Delivered curated datasets and Power BI dashboards tracking KPIs  
‚Ä¢ Reduced manual data preparation and analytics latency  

---

## üõ† Technical Skills

**Programming:**  
Python ‚Ä¢ SQL ‚Ä¢ PySpark ‚Ä¢ R  

**Cloud & Data Platforms:**  
Azure Data Factory ‚Ä¢ ADLS Gen2 ‚Ä¢ Azure Databricks ‚Ä¢ Delta Lake  
Microsoft Fabric ‚Ä¢ OneLake ‚Ä¢ Azure Synapse  

**Data Engineering:**  
ETL / ELT Pipelines ‚Ä¢ Medallion Architecture ‚Ä¢ Data Modeling  
Delta Tables ‚Ä¢ Parquet ‚Ä¢ Schema Optimization ‚Ä¢ Data Quality Validation  

**Tools & Practices:**  
Git / Version Control ‚Ä¢ ARM Templates ‚Ä¢ Notebook Workflows  
Azure Key Vault ‚Ä¢ Azure Logic Apps ‚Ä¢ Power BI  

---

## üìà Areas of Interest

Cloud Data Engineering ‚Ä¢ Lakehouse Architectures ‚Ä¢ Distributed Processing  
Data Platform Optimization ‚Ä¢ Analytics Engineering  

---

## üîó Connect With Me

üåê Portfolio: https://chaitanyakarthik.github.io/Portfolio-web/  
üíº LinkedIn:  https://www.linkedin.com/in/chaitanya-karthik-t/


---

Thanks for visiting my GitHub üöÄ
